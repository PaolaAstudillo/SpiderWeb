import asyncio
import logging
from collections import defaultdict
from urllib.parse import urljoin, urlparse
from asgiref.sync import sync_to_async
from django.utils.timezone import now
import requests
from scrapy import signals, Spider, Request
from monitoring_app.models import Pagina, Enlace
import httpx
from django.db.models import Count
import signal
import http.client
import pycurl
from io import BytesIO
from fake_useragent import UserAgent
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

logger = logging.getLogger(__name__)

class EstadoHTTPDetector:
    def __init__(self, url):
        self.url = url

    def detectar_estado_http_selenium(self):
        """Detecta el estado HTTP usando requests y Selenium."""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        service = Service('C:/chromedriver/chromedriver.exe')
        driver = None

        try:
            # Verificar el estado HTTP con requests primero
            response = requests.get(self.url, timeout=30, verify=False)
            status_code = response.status_code
            logger.info(f"üåê Estado HTTP de {self.url}: {status_code}")

            if status_code == 200:
                # Si el estado HTTP es exitoso, usar Selenium para verificar el contenido
                driver = webdriver.Chrome(service=service, options=chrome_options)
                driver.get(self.url)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

                # Verificar contenido de la p√°gina para estados espec√≠ficos
                page_source = driver.page_source
                if "404" in driver.title or "Page Not Found" in page_source:
                    logger.warning(f"üö´ P√°gina no encontrada (404): {self.url}")
                    return 404
                elif "500" in driver.title or "Server Error" in page_source:
                    logger.error(f"üí• Error del servidor (500): {self.url}")
                    return 500
                else:
                    logger.info(f"‚úÖ P√°gina cargada correctamente: {self.url}")
                    return 200
            else:
                logger.warning(f"‚ö†Ô∏è La solicitud a {self.url} devolvi√≥ el c√≥digo {status_code}")
                return status_code
        except requests.exceptions.Timeout:
            logger.error(f"‚è≥ Timeout al conectar con {self.url}")
            return 408  # Request Timeout
        except requests.exceptions.ConnectionError:
            logger.error(f"üîå Error de conexi√≥n con {self.url}")
            return 503  # Service Unavailable
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None
        finally:
            if driver:
                driver.quit()   


    async def detectar_estado_http_requests(self):
        """Detecta el estado HTTP usando httpx con manejo avanzado de errores."""
        try:
            headers = {'User-Agent': UserAgent().random}
            async with httpx.AsyncClient(timeout=10, follow_redirects=True, headers=headers, verify=False) as client:
                response = await client.get(self.url)
                return response
        except httpx.TimeoutException:
            logger.error(f"‚è≥ Timeout en {self.url}")
        except httpx.HTTPStatusError as e:
            logger.error(f"‚ö†Ô∏è Error HTTP en {self.url}: {e.response.status_code}")
        except httpx.RequestError as e:
            logger.error(f"‚ùå Error de conexi√≥n en {self.url}: {e}")
        return None

    def detectar_estado_http_http_client(self):
        """Detecta el estado HTTP usando http.client."""
        try:
            # Parsear la URL para extraer esquema, host y ruta
            parsed_url = urlparse(self.url)
            scheme = parsed_url.scheme
            netloc = parsed_url.netloc
            path = parsed_url.path or "/"

            # Verificar que la URL tenga un esquema y un host v√°lidos
            if not scheme or not netloc:
                logger.error(f"‚ùå URL inv√°lida: {self.url}")
                return None

            # Configurar la conexi√≥n seg√∫n el esquema
            conn = (http.client.HTTPSConnection(netloc) if scheme == "https" 
                    else http.client.HTTPConnection(netloc))

            # Realizar la solicitud HTTP GET
            logger.info(f"üåê Conectando a {scheme}://{netloc}{path}")
            conn.request("GET", path)
            response = conn.getresponse()
            status_code = response.status
            reason = response.reason

            # Loguear el resultado
            logger.info(f"‚úÖ Respuesta recibida: {status_code} {reason}")
            return status_code
        except http.client.HTTPException as http_err:
            logger.error(f"‚ùå Error de protocolo HTTP: {http_err}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None
        finally:
            try:
                # Asegurar el cierre de la conexi√≥n
                if 'conn' in locals() and conn:
                    conn.close()
                    logger.info("üîí Conexi√≥n cerrada correctamente")
            except Exception as close_err:
                logger.warning(f"‚ö†Ô∏è Error al cerrar la conexi√≥n: {close_err}")


    def detectar_estado_http_pycurl(self):
        """Detecta el estado HTTP usando PyCurl"""
        buffer = BytesIO()
        c = pycurl.Curl()

        try:
            # Configuraci√≥n de PyCurl
            logger.info(f"üåê Conectando a {self.url} usando PyCurl")
            c.setopt(c.URL, self.url)
            c.setopt(c.WRITEDATA, buffer)  # Donde almacenar la respuesta
            c.setopt(c.FOLLOWLOCATION, True)  # Seguir redirecciones
            c.setopt(c.TIMEOUT, 10)  # Timeout de la conexi√≥n
            c.setopt(c.SSL_VERIFYPEER, 0)  # Deshabilitar la verificaci√≥n SSL (no recomendado en producci√≥n)
            c.setopt(c.SSL_VERIFYHOST, 0)  # Deshabilitar la verificaci√≥n del host

            # Realizar la solicitud
            c.perform()

            # Obtener el c√≥digo de estado HTTP
            status_code = c.getinfo(c.RESPONSE_CODE)
            logger.info(f"‚úÖ Respuesta recibida: {status_code}")
            return status_code
        except pycurl.error as curl_err:
            logger.error(f"‚ùå Error de PyCurl: {curl_err}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None
        finally:
            try:
                c.close()
                logger.info("üîí Conexi√≥n cerrada correctamente con PyCurl")
            except Exception as close_err:
                logger.warning(f"‚ö†Ô∏è Error al cerrar la conexi√≥n de PyCurl: {close_err}")

    def detectar_estado_http_300(url):
        """
        Detecta el estado HTTP de una URL, manejando espec√≠ficamente los c√≥digos de redireccionamiento (3xx).
        
        :param url: La URL a la que se desea hacer la solicitud.
        :return: El c√≥digo de estado HTTP final despu√©s de seguir las redirecciones, o None si ocurre un error.
        """
        try:
            # Realizar la solicitud HTTP GET con redirecciones autom√°ticas
            response = requests.get(url, timeout=30, allow_redirects=True)
            
            # Obtener el c√≥digo de estado HTTP final
            status_code = response.status_code
            logger.info(f"üåê Estado HTTP final de {url}: {status_code}")
            
            # Si el c√≥digo de estado es 3xx, loguear la redirecci√≥n
            if 300 <= status_code < 400:
                logger.info(f"üîÄ Redireccionamiento detectado: {status_code}")
                # Obtener la URL final despu√©s de las redirecciones
                final_url = response.url
                logger.info(f"üîÄ Redireccionado a: {final_url}")
            
            return status_code
        
        except requests.exceptions.Timeout:
            logger.error(f"‚è≥ Timeout al conectar con {url}")
            return 408  # Request Timeout
        
        except requests.exceptions.ConnectionError:
            logger.error(f"üîå Error de conexi√≥n con {url}")
            return 503  # Service Unavailable
        
        except requests.exceptions.TooManyRedirects:
            logger.error(f"üîÄ Demasiadas redirecciones en {url}")
            return 310  # Too Many Redirects (c√≥digo personalizado para este caso)
        
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None

class HTTPStatusLogger:
    def __init__(self):
        self.http_status_counts = defaultdict(int)
        self.error_links = []

    def log_http_status(self, response):
        """Registra los c√≥digos HTTP y maneja cada estado de manera adecuada."""
        if response is None:
            logger.warning("‚ö†Ô∏è Respuesta sin c√≥digo HTTP.")
            return

        if hasattr(response, 'status_code'):
            status_code = response.status_code
            url = str(response.url)
        else:
            status_code = response
            url = "URL no disponible"

        self.http_status_counts[status_code] += 1

        status_handlers = {
            200: lambda: logger.info(f"‚úÖ [200] Enlace funcional: {url}"),
            301: lambda: self._handle_redirect(response, "üîÑ [301] Redirecci√≥n permanente"),
            302: lambda: self._handle_redirect(response, "üîÑ [302] Redirecci√≥n temporal"),
            307: lambda: self._handle_redirect(response, "üîÑ [307] Redirecci√≥n temporal"),
            308: lambda: self._handle_redirect(response, "üîÑ [308] Redirecci√≥n permanente"),
            400: lambda: self._handle_error_link(response, "‚ö†Ô∏è Solicitud incorrecta (400)"),
            401: lambda: self._handle_error_link(response, "üîë Autenticaci√≥n requerida (401)"),
            403: lambda: self._handle_error_link(response, "üö´ Acceso denegado (403)"),
            404: lambda: self._handle_error_link(response, "üö® Enlace roto (404)"),
            429: lambda: self._handle_error_link(response, "‚è≥ Demasiadas solicitudes (429)"),
            500: lambda: self._handle_error_link(response, "üõë Error interno del servidor (500)"),
            503: lambda: self._handle_error_link(response, "üöß Servicio no disponible (503)"),
            "4xx": lambda: self._handle_error_link(response, f"‚ö†Ô∏è Error del cliente ({status_code})"),
            "5xx": lambda: self._handle_error_link(response, f"üõë Error del servidor ({status_code})"),
            "default": lambda: logger.warning(f"üî∂ [{status_code}] C√≥digo HTTP inesperado en {url}"),
        }

        handler = (
            status_handlers.get(status_code)
            or (status_handlers["4xx"] if 400 <= status_code < 500 else None)
            or (status_handlers["5xx"] if 500 <= status_code < 600 else None)
            or status_handlers["default"]
        )
        handler()

    def _handle_redirect(self, response, message):
        redirect_url = response.headers.get("Location", "desconocida") if hasattr(response, 'headers') else "desconocida"
        logger.info(f"{message} a {redirect_url}")

    def _handle_error_link(self, response, message):
        url = str(response.url) if hasattr(response, 'url') else "URL no disponible"
        status_code = response.status_code if hasattr(response, 'status_code') else response
        self.error_links.append((url, status_code))
        logger.error(f"{message} [{status_code}]: {url}")

class SpiderSitio(Spider):
    name = 'spider_sitio'

    def __init__(self, dominio=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not dominio:
            raise ValueError("‚ùå Debes especificar un dominio con -a dominio=<dominio>")

        parsed = urlparse(dominio if dominio.startswith("http") else f"https://{dominio}")
        if not parsed.netloc:
            raise ValueError(f"‚ùå Dominio inv√°lido: {dominio}")

        self.start_urls = [parsed.geturl()]
        self.allowed_domains = [parsed.netloc]
        self.visited = set()
        self.discovered = set()
        self.http_status_logger = HTTPStatusLogger()
        self.tasks = []
        logger.info(f"üöÄ Spider inicializado para el dominio: {dominio}")

        signal.signal(signal.SIGINT, self.handle_interrupt)
        signal.signal(signal.SIGTERM, self.handle_interrupt)

    def handle_interrupt(self, signum, frame):
        """Maneja SIGINT  y SIGTERM para cerrar adecuadamente el Spider."""
        logger.warning(f"üõë Se√±al de interrupci√≥n recibida ({signum}). Deteniendo el Spider...")
        self.crawler.engine.close_spider(self, "interrupted")

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
        return spider

    async def _save_pagina_data(self, url, status):
        try:
            pagina, created = await sync_to_async(Pagina.objects.get_or_create)(url=url, defaults={"created_at": now()})
            await sync_to_async(pagina.actualizar_ultima_consulta)(status)
            logger.info(f"üíæ P√°gina guardada: {url} (creada: {created})")
            return pagina
        except Exception as e:
            logger.error(f"‚ùå Error al guardar datos de la p√°gina {url}: {e}")
            return None

    async def _save_enlace_data(self, origen, destino):
        try:
            _, enlace_creado = await sync_to_async(Enlace.objects.get_or_create)(pagina_origen=origen, pagina_destino=destino)
            if enlace_creado:
                logger.info(f"üîó Enlace creado: {origen.url} -> {destino.url}")
                await self._check_if_huerfana(destino)
            return enlace_creado
        except Exception as e:
            logger.error(f"‚ùå Error al guardar enlace entre {origen.url} y {destino.url}: {e}")
            return False

    async def _check_if_huerfana(self, pagina):
        try:
            is_huerfana = await sync_to_async(
                lambda: Pagina.objects.filter(id=pagina.id, enlaces_entrantes__isnull=True).exists()
            )()
            if is_huerfana:
                await sync_to_async(pagina.marcar_huerfana)()
                logger.warning(f"üëÄ P√°gina hu√©rfana detectada: {pagina.url}")
        except Exception as e:
            logger.error(f"‚ùå Error comprobando si la p√°gina es hu√©rfana: {e}")

    async def _detect_huerfanas(self):
        try:
            paginas_huerfanas = await sync_to_async(lambda: list(
                Pagina.objects.annotate(num_enlaces=Count('enlaces_entrantes'))
                              .filter(num_enlaces=0)
                              .exclude(url__in=self.start_urls)
                              .only("url")
            ))()
            for pagina in paginas_huerfanas:
                await sync_to_async(pagina.marcar_huerfana)()
                logger.warning(f"üëÄ P√°gina hu√©rfana detectada: {pagina.url}")
        except Exception as e:
            logger.error(f"‚ùå Error detectando p√°ginas hu√©rfanas: {e}")

    async def parse(self, response):
        try:
            if response.url in self.visited:
                return

            self.visited.add(response.url)
            logger.info(f"üîé Visitando: {response.url}")
            logger.info(f"Visiting {response.url}")  # Log para la vista de Django

            # Detectar c√≥digo HTTP usando m√∫ltiples m√©todos
            detector = EstadoHTTPDetector(response.url)
            tasks = [
                detector.detectar_estado_http_requests(),
                sync_to_async(detector.detectar_estado_http_selenium)(),
                sync_to_async(detector.detectar_estado_http_http_client)(),
                sync_to_async(detector.detectar_estado_http_pycurl)(),
                sync_to_async(detector.detectar_estado_http_300)()

              
            ]
           
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"‚ùå Error en una de las tareas: {result}")
                elif result is not None:
                    self.http_status_logger.log_http_status(result)

            # Registrar en la BD
            pagina_actual = await self._save_pagina_data(response.url, response.status)

            # Usar BeautifulSoup para extraer enlaces de p√°ginas din√°micas
            soup = BeautifulSoup(response.text, 'html.parser')
            enlaces = [a['href'] for a in soup.find_all('a', href=True)]
            logger.info(f"üîó Enlaces encontrados en {response.url}: {len(enlaces)}")

            for enlace in enlaces:
                enlace_completo = urljoin(response.url, enlace)
                parsed_enlace = urlparse(enlace_completo)
                if parsed_enlace.netloc not in self.allowed_domains:
                    logger.info(f"üö´ Enlace fuera del dominio permitido: {enlace_completo}")
                    continue

                if enlace_completo in self.visited or enlace_completo == response.url:
                    continue

                self.discovered.add(enlace_completo)
                logger.info(f"üîó Nuevo enlace descubierto: {enlace_completo}")
                logger.info(f"Discovered {enlace_completo}")  # Log para la vista de Django

                pagina_destino = await self._save_pagina_data(enlace_completo, None)
                await self._save_enlace_data(pagina_actual, pagina_destino)

                yield Request(enlace_completo, callback=self.parse)

        except Exception as e:
            logger.error(f"‚ùå Error procesando {response.url}: {e}")

    async def spider_closed(self, reason):
        logger.info("üèÅ Spider finalizado.")
        logger.info(f"üìä Total de p√°ginas visitadas: {len(self.visited)}")
        logger.info(f"üìà Resumen de c√≥digos HTTP: {dict(self.http_status_logger.http_status_counts)}")

        if self.http_status_logger.error_links:
            logger.error(f"üî¥ Errores en {len(self.http_status_logger.error_links)} enlaces:")
            for url, status in self.http_status_logger.error_links:
                logger.error(f"‚ùå [{status}] {url}")

        unvisited = self.discovered - self.visited
        if unvisited:
            logger.info(f"üîç Enlaces descubiertos pero no visitados: {len(unvisited)}")
            for url in unvisited:
                logger.info(f"üîó {url}")

        if self.tasks:
            await asyncio.wait(self.tasks)
            logger.info("‚úÖ Todas las tareas de guardado han finalizado.")

        await self._detect_huerfanas()  # Detectar p√°ginas hu√©rfanas antes de cerrar