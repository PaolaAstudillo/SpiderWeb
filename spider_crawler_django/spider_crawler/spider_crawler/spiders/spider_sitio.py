import asyncio
import logging
from collections import defaultdict
import re
from urllib.parse import urljoin, urlparse
from asgiref.sync import sync_to_async
from django.utils.timezone import now
import requests
from scrapy import signals, Spider, Request
from monitoring_app.models import EnlaceRoto, Pagina, Enlace
import httpx
from django.db.models import Count
import signal
import http.client
import pycurl
from io import BytesIO
from fake_useragent import UserAgent
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import unquote


logger = logging.getLogger(__name__)

class EstadoHTTPDetector:
    def __init__(self, url):
        self.url = url

    def detectar_estado_http_selenium(self):
        """Detecta el estado HTTP usando requests y Selenium."""
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        service = Service('C:/chromedriver/chromedriver.exe')
        driver = None

        try:
            # Verificar el estado HTTP con requests primero
            response = requests.get(self.url, timeout=30, verify=False)
            status_code = response.status_code
            logger.info(f"üåê Estado HTTP de {self.url}: {status_code}")

            if status_code == 200:
                # Si el estado HTTP es exitoso, usar Selenium para verificar el contenido
                driver = webdriver.Chrome(service=service, options=chrome_options)
                driver.get(self.url)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

                # Verificar contenido de la p√°gina para estados espec√≠ficos
                page_source = driver.page_source
                if "404" in driver.title or "Page Not Found" in page_source:
                    logger.warning(f"üö´ P√°gina no encontrada (404): {self.url}")
                    return 404
                elif "500" in driver.title or "Server Error" in page_source:
                    logger.error(f"üí• Error del servidor (500): {self.url}")
                    return 500
                else:
                    logger.info(f"‚úÖ P√°gina cargada correctamente: {self.url}")
                    return 200
            else:
                logger.warning(f"‚ö†Ô∏è La solicitud a {self.url} devolvi√≥ el c√≥digo {status_code}")
                return status_code
        except requests.exceptions.Timeout:
            logger.error(f"‚è≥ Timeout al conectar con {self.url}")
            return 408  # Request Timeout
        except requests.exceptions.ConnectionError:
            logger.error(f"üîå Error de conexi√≥n con {self.url}")
            return 503  # Service Unavailable
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None
        finally:
            if driver:
                driver.quit()   


    async def detectar_estado_http_requests(self):
        """Detecta el estado HTTP usando httpx con manejo avanzado de errores."""
        try:
            headers = {'User-Agent': UserAgent().random}
            async with httpx.AsyncClient(timeout=10, follow_redirects=True, headers=headers, verify=False) as client:
                response = await client.get(self.url)
                return response.status_code  # Devuelve el c√≥digo de estado HTTP
        except httpx.TimeoutException:
            logger.error(f"‚è≥ Timeout en {self.url}")
            return 408  # Request Timeout
        except httpx.HTTPStatusError as e:
            logger.error(f"‚ö†Ô∏è Error HTTP en {self.url}: {e.response.status_code}")
            return e.response.status_code
        except httpx.RequestError as e:
            logger.error(f"‚ùå Error de conexi√≥n en {self.url}: {e}")
            return 503  # Service Unavailable

    def detectar_estado_http_http_client(self):
        """Detecta el estado HTTP usando http.client."""
        try:
            # Parsear la URL para extraer esquema, host y ruta
            parsed_url = urlparse(self.url)
            scheme = parsed_url.scheme
            netloc = parsed_url.netloc
            path = parsed_url.path or "/"

            # Verificar que la URL tenga un esquema y un host v√°lidos
            if not scheme or not netloc:
                logger.error(f"‚ùå URL inv√°lida: {self.url}")
                return None

            # Configurar la conexi√≥n seg√∫n el esquema
            conn = (http.client.HTTPSConnection(netloc) if scheme == "https" 
                    else http.client.HTTPConnection(netloc))

            # Realizar la solicitud HTTP GET
            logger.info(f"üåê Conectando a {scheme}://{netloc}{path}")
            conn.request("GET", path)
            response = conn.getresponse()
            status_code = response.status
            reason = response.reason

            # Loguear el resultado
            logger.info(f"‚úÖ Respuesta recibida: {status_code} {reason}")
            return status_code
        except http.client.HTTPException as http_err:
            logger.error(f"‚ùå Error de protocolo HTTP: {http_err}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None
        finally:
            try:
                # Asegurar el cierre de la conexi√≥n
                if 'conn' in locals() and conn:
                    conn.close()
                    logger.info("üîí Conexi√≥n cerrada correctamente")
            except Exception as close_err:
                logger.warning(f"‚ö†Ô∏è Error al cerrar la conexi√≥n: {close_err}")


    def detectar_estado_http_pycurl(self):
        """Detecta el estado HTTP usando PyCurl"""
        buffer = BytesIO()
        c = pycurl.Curl()

        try:
            # Configuraci√≥n de PyCurl
            logger.info(f"üåê Conectando a {self.url} usando PyCurl")
            c.setopt(c.URL, self.url)
            c.setopt(c.WRITEDATA, buffer)  # Donde almacenar la respuesta
            c.setopt(c.FOLLOWLOCATION, True)  # Seguir redirecciones
            c.setopt(c.TIMEOUT, 10)  # Timeout de la conexi√≥n
            c.setopt(c.SSL_VERIFYPEER, 0)  # Deshabilitar la verificaci√≥n SSL (no recomendado en producci√≥n)
            c.setopt(c.SSL_VERIFYHOST, 0)  # Deshabilitar la verificaci√≥n del host

            # Realizar la solicitud
            c.perform()

            # Obtener el c√≥digo de estado HTTP
            status_code = c.getinfo(c.RESPONSE_CODE)
            logger.info(f"‚úÖ Respuesta recibida: {status_code}")
            return status_code
        except pycurl.error as curl_err:
            logger.error(f"‚ùå Error de PyCurl: {curl_err}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None
        finally:
            try:
                c.close()
                logger.info("üîí Conexi√≥n cerrada correctamente con PyCurl")
            except Exception as close_err:
                logger.warning(f"‚ö†Ô∏è Error al cerrar la conexi√≥n de PyCurl: {close_err}")



    async def detectar_estado_http_300(self):
        """
        Detecta el estado HTTP de una URL, manejando espec√≠ficamente los c√≥digos de redireccionamiento (3xx).
        Adem√°s, detecta redirecciones que involucran cambios en la codificaci√≥n de la URL.

        :return: El c√≥digo de estado HTTP final despu√©s de seguir las redirecciones, o None si ocurre un error.
        """
        try:
            # Realizar la solicitud HTTP GET con redirecciones autom√°ticas
            async with httpx.AsyncClient() as client:
                response = await client.get(self.url, follow_redirects=True, timeout=30)
            
            # Obtener el c√≥digo de estado HTTP final
            status_code = response.status_code
            logger.info(f"üåê Estado HTTP final de {self.url}: {status_code}")
            
            # Si el c√≥digo de estado es 3xx, loguear la redirecci√≥n
            if 300 <= status_code < 400:
                logger.info(f"üîÄ Redireccionamiento detectado: {status_code}")
                # Obtener la URL final despu√©s de las redirecciones
                final_url = str(response.url)
                logger.info(f"üîÄ Redireccionado a: {final_url}")
                
                # Comparar la URL original con la URL final para detectar cambios en la codificaci√≥n
                original_parsed = urlparse(self.url)
                final_parsed = urlparse(final_url)
                
                # Decodificar las partes de la URL para compararlas
                original_path = unquote(original_parsed.path)
                final_path = unquote(final_parsed.path)
                
                if original_path != final_path:
                    logger.info(f"üîÄ Cambio en la codificaci√≥n de la URL detectado: {original_path} -> {final_path}")
            
            return status_code
        
        except httpx.TimeoutException:
            logger.error(f"‚è≥ Timeout al conectar con {self.url}")
            return 408  # Request Timeout
        
        except httpx.RequestError as e:
            logger.error(f"üîå Error de conexi√≥n con {self.url}: {e}")
            return 503  # Service Unavailable
        
        except Exception as e:
            logger.error(f"‚ùå Error inesperado: {e}")
            return None


    async def detectar_estado_404(self):
        """
        Detecta si una p√°gina devuelve un error 404, ya sea por c√≥digo de estado o por contenido.
        
        :return: El c√≥digo de estado HTTP (404 si se detecta, None si no).
        """
        try:
            # Realizar la solicitud HTTP
            async with httpx.AsyncClient() as client:
                response = await client.get(self.url, timeout=30)
            
            # Verificar el c√≥digo de estado HTTP
            if response.status_code == 404:
                logger.warning(f"üö´ 404 detectado por c√≥digo de estado en {self.url}")
                return 404
            
            # Analizar el contenido HTML para buscar indicadores comunes de 404
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar en el t√≠tulo
            title = soup.find('title')
            if title and re.search(r'404|not found|page not found', title.text, re.IGNORECASE):
                logger.warning(f"üö´ 404 detectado en el t√≠tulo de la p√°gina: {self.url}")
                return 404
            
            # Buscar en elementos span u otros que puedan contener mensajes de error
            error_messages = soup.find_all(text=re.compile(r'404|not found|page not found', re.IGNORECASE))
            if error_messages:
                logger.warning(f"üö´ 404 detectado en el contenido de la p√°gina: {self.url}")
                return 404
            
            # Si no se encuentra ning√∫n indicador de 404
            logger.info(f"‚úÖ No se detect√≥ ning√∫n error 404 en {self.url}")
            return None
        
        except httpx.TimeoutException:
            logger.error(f"‚è≥ Timeout al conectar con {self.url}")
            return 408  # Request Timeout
        
        except httpx.RequestError as e:
            logger.error(f"üîå Error de conexi√≥n con {self.url}: {e}")
            return 503  # Service Unavailable
        
        except Exception as e:
            logger.error(f"‚ùå Error inesperado en {self.url}: {e}")
            return None




class HTTPStatusLogger:
    def __init__(self):
        self.http_status_counts = defaultdict(int)
        self.error_links = []

  

    def log_http_status(self, response, url=None):
        """Registra los c√≥digos HTTP y maneja cada estado de manera adecuada."""
        if response is None:
            logger.warning("‚ö†Ô∏è Respuesta sin c√≥digo HTTP.")
            return

        if hasattr(response, 'status_code'):
            status_code = response.status_code
            url = str(response.url) if hasattr(response, 'url') else url
        else:
            status_code = response
            url = url or "URL no disponible"

        self.http_status_counts[status_code] += 1

        status_handlers = {
            200: lambda: logger.info(f"‚úÖ [200] Enlace funcional: {url}"),
            301: lambda: self._handle_redirect(response, "üîÑ [301] Redirecci√≥n permanente"),
            302: lambda: self._handle_redirect(response, "üîÑ [302] Redirecci√≥n temporal"),
            307: lambda: self._handle_redirect(response, "üîÑ [307] Redirecci√≥n temporal"),
            308: lambda: self._handle_redirect(response, "üîÑ [308] Redirecci√≥n permanente"),
            400: lambda: self._handle_error_link(response, "‚ö†Ô∏è Solicitud incorrecta (400)"),
            401: lambda: self._handle_error_link(response, "üîë Autenticaci√≥n requerida (401)"),
            403: lambda: self._handle_error_link(response, "üö´ Acceso denegado (403)"),
            404: lambda: self._handle_error_link(response, "üö® Enlace roto (404)"),
            429: lambda: self._handle_error_link(response, "‚è≥ Demasiadas solicitudes (429)"),
            500: lambda: self._handle_error_link(response, "üõë Error interno del servidor (500)"),
            503: lambda: self._handle_error_link(response, "üöß Servicio no disponible (503)"),
            "4xx": lambda: self._handle_error_link(response, f"‚ö†Ô∏è Error del cliente ({status_code})"),
            "5xx": lambda: self._handle_error_link(response, f"üõë Error del servidor ({status_code})"),
            "default": lambda: logger.warning(f"üî∂ [{status_code}] C√≥digo HTTP inesperado en {url}"),
        }

        handler = (
            status_handlers.get(status_code)
            or (status_handlers["4xx"] if 400 <= status_code < 500 else None)
            or (status_handlers["5xx"] if 500 <= status_code < 600 else None)
            or status_handlers["default"]
        )
        handler()




    def _handle_redirect(self, response, message):
        redirect_url = response.headers.get("Location", "desconocida") if hasattr(response, 'headers') else "desconocida"
        logger.info(f"{message} a {redirect_url}")

    def _handle_error_link(self, response, message):
        url = str(response.url) if hasattr(response, 'url') else "URL no disponible"
        status_code = response.status_code if hasattr(response, 'status_code') else response
        self.error_links.append((url, status_code))
        logger.error(f"{message} [{status_code}]: {url}")

class SpiderSitio(Spider):
    name = 'spider_sitio'

    def __init__(self, dominio=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not dominio:
            raise ValueError("‚ùå Debes especificar un dominio con -a dominio=<dominio>")

        parsed = urlparse(dominio if dominio.startswith("http") else f"https://{dominio}")
        if not parsed.netloc:
            raise ValueError(f"‚ùå Dominio inv√°lido: {dominio}")

        self.start_urls = [parsed.geturl()]
        self.allowed_domains = [parsed.netloc]
        self.visited = set()
        self.discovered = set()
        self.http_status_logger = HTTPStatusLogger()
        self.tasks = []
        logger.info(f"üöÄ Spider inicializado para el dominio: {dominio}")

        signal.signal(signal.SIGINT, self.handle_interrupt)
        signal.signal(signal.SIGTERM, self.handle_interrupt)

    def handle_interrupt(self, signum, frame):
        """Maneja SIGINT  y SIGTERM para cerrar adecuadamente el Spider."""
        logger.warning(f"üõë Se√±al de interrupci√≥n recibida ({signum}). Deteniendo el Spider...")
        self.crawler.engine.close_spider(self, "interrupted")



    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
        return spider

    async def _save_pagina_data(self, url, status):
        try:
            # Guardar la p√°gina y su estado HTTP
            pagina, created = await sync_to_async(Pagina.objects.get_or_create)(
                url=url,
                defaults={
                    "created_at": now(),
                    "codigo_estado": status,  # Guardar el estado HTTP
                }
            )
            if not created:
                # Si la p√°gina ya existe, actualizar el estado HTTP
                await sync_to_async(pagina.actualizar_ultima_consulta)(status)
            logger.info(f"üíæ P√°gina guardada: {url} (creada: {created})")
            return pagina
        except Exception as e:
            logger.error(f"‚ùå Error al guardar datos de la p√°gina {url}: {e}")
            return None

    async def _save_enlace_data(self, origen, destino):
        try:
            _, enlace_creado = await sync_to_async(Enlace.objects.get_or_create)(pagina_origen=origen, pagina_destino=destino)
            if enlace_creado:
                logger.info(f"üîó Enlace creado: {origen.url} -> {destino.url}")
                await self._check_if_huerfana(destino)
            return enlace_creado
        except Exception as e:
            logger.error(f"‚ùå Error al guardar enlace entre {origen.url} y {destino.url}: {e}")
            return False

    async def _check_if_huerfana(self, pagina):
        try:
            is_huerfana = await sync_to_async(
                lambda: Pagina.objects.filter(id=pagina.id, enlaces_entrantes__isnull=True).exists()
            )()
            if is_huerfana:
                await sync_to_async(pagina.marcar_huerfana)()
                logger.warning(f"üëÄ P√°gina hu√©rfana detectada: {pagina.url}")
        except Exception as e:
            logger.error(f"‚ùå Error comprobando si la p√°gina es hu√©rfana: {e}")

    async def _detect_huerfanas(self):
        try:
            paginas_huerfanas = await sync_to_async(lambda: list(
                Pagina.objects.annotate(num_enlaces=Count('enlaces_entrantes'))
                              .filter(num_enlaces=0)
                              .exclude(url__in=self.start_urls)
                              .only("url")
            ))()
            for pagina in paginas_huerfanas:
                await sync_to_async(pagina.marcar_huerfana)()
                logger.warning(f"üëÄ P√°gina hu√©rfana detectada: {pagina.url}")
        except Exception as e:
            logger.error(f"‚ùå Error detectando p√°ginas hu√©rfanas: {e}")



    async def guardar_enlace_roto(self, pagina, url_enlace_roto, codigo_estado):
        """Guarda un enlace roto en la base de datos."""
        try:
            enlace_roto, created = await sync_to_async(EnlaceRoto.objects.get_or_create)(
                pagina=pagina,
                url_enlace_roto=url_enlace_roto,
                defaults={
                    "codigo_estado": codigo_estado,
                    "detectado_en": now(),
                }
            )
            if created:
                logger.info(f"üö´ Enlace roto guardado: {url_enlace_roto} (C√≥digo: {codigo_estado}) en {pagina.url}")
            return enlace_roto
        except Exception as e:
            logger.error(f"‚ùå Error al guardar enlace roto {url_enlace_roto} en {pagina.url}: {e}")
            return None



    async def parse(self, response):
        try:
            if response.url in self.visited:
                return

            self.visited.add(response.url)
            logger.info(f"üîé Visitando: {response.url}")
            logger.info(f"Visiting {response.url}")  # Log para la vista de Django

            # Detectar c√≥digo HTTP usando m√∫ltiples m√©todos
            detector = EstadoHTTPDetector(response.url)
            tasks = [
                detector.detectar_estado_http_requests(),
                sync_to_async(detector.detectar_estado_http_selenium)(),
                sync_to_async(detector.detectar_estado_http_http_client)(),
                sync_to_async(detector.detectar_estado_http_pycurl)(),
                detector.detectar_estado_http_300(),
                detector.detectar_estado_404()


              
            ]
           
            results = await asyncio.gather(*tasks, return_exceptions=True)

            estado_http = None
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"‚ùå Error en una de las tareas: {result}")
                elif result is not None:
                    # Si el resultado es una tupla, extrae el c√≥digo de estado
                    if isinstance(result, tuple):
                        estado_http, message = result
                    else:
                        estado_http = result
                        message = "C√≥digo de estado HTTP"
                    
                    self.http_status_logger.log_http_status(estado_http, url=response.url)

            # Registrar en la BD
            pagina_actual = await self._save_pagina_data(response.url, estado_http)


            # Usar BeautifulSoup para extraer enlaces de p√°ginas est√°ticas
            
            # Analizar el contenido HTML para buscar indicadores comunes de 404
            soup = BeautifulSoup(response.text, 'html.parser')
            enlaces = [a['href'] for a in soup.find_all('a', href=True)]
            logger.info(f"üîó Enlaces encontrados en {response.url}: {len(enlaces)}")

            for enlace in enlaces:
                enlace_completo = urljoin(response.url, enlace)
                parsed_enlace = urlparse(enlace_completo)
                if parsed_enlace.netloc not in self.allowed_domains:
                    logger.info(f"üö´ Enlace fuera del dominio permitido: {enlace_completo}")
                    continue

                if enlace_completo in self.visited or enlace_completo == response.url:
                    continue

                self.discovered.add(enlace_completo)
                logger.info(f"üîó Nuevo enlace descubierto: {enlace_completo}")
                logger.info(f"Discovered {enlace_completo}")  # Log para la vista de Django

                pagina_destino = await self._save_pagina_data(enlace_completo, None)
                await self._save_enlace_data(pagina_actual, pagina_destino)

                yield Request(enlace_completo, callback=self.parse)

        except Exception as e:
            logger.error(f"‚ùå Error procesando {response.url}: {e}")

    async def spider_closed(self, reason):
        logger.info("üèÅ Spider finalizado.")
        logger.info(f"üìä Total de p√°ginas visitadas: {len(self.visited)}")
        logger.info(f"üìà Resumen de c√≥digos HTTP: {dict(self.http_status_logger.http_status_counts)}")

        if self.http_status_logger.error_links:
            logger.error(f"üî¥ Errores en {len(self.http_status_logger.error_links)} enlaces:")
            for url, status in self.http_status_logger.error_links:
                logger.error(f"‚ùå [{status}] {url}")

        unvisited = self.discovered - self.visited
        if unvisited:
            logger.info(f"üîç Enlaces descubiertos pero no visitados: {len(unvisited)}")
            for url in unvisited:
                logger.info(f"üîó {url}")

        if self.tasks:
            await asyncio.wait(self.tasks)
            logger.info("‚úÖ Todas las tareas de guardado han finalizado.")

        await self._detect_huerfanas()  # Detectar p√°ginas hu√©rfanas antes de cerrarAA
        
        
        
        
